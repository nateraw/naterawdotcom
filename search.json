[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nateraw.com",
    "section": "",
    "text": "Hi, I‚Äôm Nate Raw. üëã\nI‚Äôm a machine learning hacker passionate about building cool products with technology.\nFeel free to reach out to me on Twitter or LinkedIn if you‚Äôd like to chat about anything or want to share an opportunity!"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "nateraw.com",
    "section": "Recent Posts",
    "text": "Recent Posts"
  },
  {
    "objectID": "posts/find_a_gpu_modal.html",
    "href": "posts/find_a_gpu_modal.html",
    "title": "Text Yourself When GPUs Are Available",
    "section": "",
    "text": "GPUs - so hot right now! It‚Äôs getting to the point where all available GPU instances are unavailable, which is a bummer if you‚Äôre trying to train a model.\nAs of writing this, Lambda Cloud is my favorite cloud GPU provider. They have great instances, an easy to use UI, and the best prices. The only downside is that they‚Äôre often sold out. So, let‚Äôs see what we can do about that."
  },
  {
    "objectID": "posts/find_a_gpu_modal.html#the-plan",
    "href": "posts/find_a_gpu_modal.html#the-plan",
    "title": "Text Yourself When GPUs Are Available",
    "section": "The Plan",
    "text": "The Plan\nTo make things easier, I put together a small Python library called lambdacloud that makes it easy to interface with the Lambda Cloud API. We‚Äôll use this to check for available instances and spin them up.\nWe‚Äôll also use Twilio to send text messages. You‚Äôll need a Twilio account and a phone number that can send SMS messages.\nTo avoid having to run this script all the time from your local machine, we‚Äôll use Modal to run the script on a schedule. Modal is a cloud orchestration platform that makes it easy to run code on cloud machines. Using it for this use case shouldn‚Äôt cost more than a few cents, but you can just follow along with the code and run it locally if you prefer.\n\n! pip install lambdacloud twilio modal-client\nSource: Find a gpu on modal\nYou‚Äôll need to authenticate with Modal if you haven‚Äôt already‚Ä¶\n\n! modal token new\nSource: Find a gpu on modal\nTo authenticate with Lambda‚Äôs API, you‚Äôll need an API key, which you can generate here when you‚Äôre logged in.\nAdditionally, you‚Äôll need Twilio‚Äôs account identifier, auth token, and phone number, which you can find here. The phone number will be the one you‚Äôll receive texts from, and it should be fairly easy to figure out how to set that up if you haven‚Äôt already.\nNow that we have all our credentials, we‚Äôll log into Modal and create a collection of secrets. See the docs on this feature here.\nI‚Äôve named my collection ‚Äútwilio‚Äù and added the following environment variables: - TWILIO_SID: Twilio account identifier - TWILIO_AUTH: Your Twilio auth token - TWILIO_PHONE: Your Twilio phone number (Make sure to include the country code, e.g.¬†+1 for US) - TO_PHONE: Your phone number you want to receive texts on (Make sure to include the country code, e.g.¬†+1 for US) - LAMBDA_SECRET: Your Lambda Labs Cloud API Key\n\n\n\nModal Secrets"
  },
  {
    "objectID": "posts/find_a_gpu_modal.html#the-code",
    "href": "posts/find_a_gpu_modal.html#the-code",
    "title": "Text Yourself When GPUs Are Available",
    "section": "The Code",
    "text": "The Code\nEverything‚Äôs ready, we just have to write a small script now üöÄ. Here it is:\n\nimport os\n\nimport modal\n\n\nstub = modal.Stub()\n\n# Defines our environment, installs necessary packages\nmy_image = modal.Image.debian_slim().pip_install(\"lambdacloud\", \"twilio\")\n\n# Replace these with your own values\nDESIRED_INSTANCE_TYPES = [\"gpu_8x_a100_80gb_sxm4\", \"gpu_8x_a100\", \"gpu_8x_v100\"]\n\n\n@stub.function(image=my_image, schedule=modal.Cron(\"*/5 3-9 * * 1-5\"), secret=modal.Secret.from_name(\"twilio\"))\ndef poll_lambda_for_big_instances():\n    from lambdacloud import list_instance_types, login\n    from twilio.rest import Client\n\n    # Auth with lambda\n    login(token=os.environ[\"LAMBDA_SECRET\"])\n\n    # Auth with twilio\n    account_sid = os.environ[\"TWILIO_SID\"]\n    auth_token = os.environ[\"TWILIO_AUTH\"]\n    client = Client(account_sid, auth_token)\n\n    from_phone = os.environ[\"TWILIO_PHONE\"]\n    to_phone = os.environ[\"TO_PHONE\"]\n\n    instances_available = [x.name for x in list_instance_types()]\n    nl = \"\\n\"\n    print(f\"Instances available:{nl}‚úÖ - {f'{nl}‚úÖ - '.join(instances_available)}\")\n\n    desired_instances_available = []\n    for desired_instance in DESIRED_INSTANCE_TYPES:\n        if desired_instance in instances_available:\n            desired_instances_available.append(desired_instance)\n\n    if len(desired_instances_available):\n        body = f\"The following instances are available on Lambda Cloud: {', '.join(desired_instances_available)}.\"\n        message = client.messages.create(from_=from_phone, to=to_phone, body=body)\n        print(f\"Message sent - SID: {message.sid}\")\n\nif __name__ == \"__main__\":\n    modal.runner.deploy_stub(stub, name=\"lambda-watcher\")\nSource: Find a gpu on modal\nIf all went well, you should get a text message when an instance becomes available. You can also check the logs in Modal to see what‚Äôs going on. In the UI, you should see something like this for the deployment:\n\n\n\nModal Logs\n\n\nThat‚Äôs it! Now you can text yourself when GPUs are available. It‚Äôs fairly easy to extend this script to spin up instances that are available, just use the lambdacloud.create_instance function. I‚Äôll leave that to you as homework üòé."
  },
  {
    "objectID": "posts/sd_music_videos.html",
    "href": "posts/sd_music_videos.html",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "",
    "text": "%%capture\n! pip install diffusers==0.4.0 transformers\nToday, we‚Äôll talk about how we can leverage Stable Diffusion to generate captivating music videos that move to the beat of a song.\nThe point of this notebook is to learn how this process works.\nIf you don‚Äôt care about the details, you can just go check out the Stable Diffusion Videos repo instead, where all this work is nicely wrapped up for you already.\nIf you like this notebook:"
  },
  {
    "objectID": "posts/sd_music_videos.html#interpolating-between-noise-vectors",
    "href": "posts/sd_music_videos.html#interpolating-between-noise-vectors",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Interpolating Between Noise Vectors",
    "text": "Interpolating Between Noise Vectors\nNow let‚Äôs interpolate between images A and B as we described earlier‚Ä¶\n\nheight = 512\nwidth = 512\nnoise_shape = (1, 4, height // 8, width // 8)\n\nseed_a = 42\nseed_b = 5432\n\nnoise_a = torch.randn(\n    noise_shape,\n    generator=torch.Generator(\n        device=pipeline.device\n    ).manual_seed(seed_a),\n    device=pipeline.device,\n)\nnoise_b = torch.randn(\n    noise_shape,\n    generator=torch.Generator(\n        device=pipeline.device\n    ).manual_seed(seed_b),\n    device=pipeline.device\n)\n\n\n# Using same prompt for each image\nprompt = 'blueberry spaghetti'\n\n# Steps to interpolate (i.e. number of images to generate)\n# If you change the number here, the visualization below will break\nnum_interpolation_steps = 6\n\nT = torch.linspace(0.0, 1.0, num_interpolation_steps)\n\nimages = []\nfor i, t in enumerate(T):\n    print(f\"Weight at step {i} - {t:2.2f}\")\n    noise_t = slerp(float(t), noise_a, noise_b)\n    im = pipeline(prompt, latents=noise_t, height=height, width=width)['sample'][0]\n    images.append(im)\n\nLet‚Äôs visualize the frames we just created‚Ä¶\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nrows, cols = 2, 3\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15,15))\n\nfor r in range(rows):\n    for c in range(cols):\n        i = (r * cols) + c\n        axes[r, c].axis(\"off\")\n        axes[r, c].imshow(images[i])\n        axes[r, c].text(0, 0, f\"{float(T[i]):2.2f}\", fontsize=24)\n\nplt.subplots_adjust(wspace=.05, hspace=.05)\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\nCool! Looks like we got an image in between A and B that seems to makes sense. Let‚Äôs do more steps and see what that looks like‚Ä¶\nThis time, we‚Äôll save images as we go, and then stitch them back together as a gif with ffmpeg.\n\nfrom pathlib import Path\n\noutput_dir = Path('images')\noutput_dir.mkdir(exist_ok=True, parents=True)\n\n# Using same prompt for each image\nprompt = 'blueberry spaghetti'\n\n# Steps to interpolate (i.e. number of images to generate)\nnum_interpolation_steps = 10\nT = torch.linspace(0.0, 1.0, num_interpolation_steps)\n\nimages = []\nfor i, t in enumerate(T):\n    noise_t = slerp(float(t), noise_a, noise_b)\n    im = pipeline(prompt, latents=noise_t, height=height, width=width)['sample'][0]\n    im.save(output_dir / f'frame{i:06d}.png')\n\nUsing ffmpeg, we can bring these together as a clip.\nHere, we‚Äôll lower the frame rate to 5 frames per second, so we should end up with a 2 second clip containing our 10 generating image frames.\n\n! ffmpeg -f image2 -framerate 5 -i images/frame%06d.png -loop 0 output_1.gif\n\nIt should look like this - pretty cool!\n\n\n\noutput_1.gif"
  },
  {
    "objectID": "posts/sd_music_videos.html#interpolating-noise-and-text-embedding-vectors",
    "href": "posts/sd_music_videos.html#interpolating-noise-and-text-embedding-vectors",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Interpolating Noise and Text Embedding Vectors",
    "text": "Interpolating Noise and Text Embedding Vectors\nTo ramp it up a notch, lets see if we can interpolate between images with different prompts. To do that, we‚Äôll need to modify the diffusers StableDiffusionPipeline to accept text embeddings, since we‚Äôll want to provide intermediate text embeddings.\nThe main bit we‚Äôre adding is this snippet, where we allow for a text_embeddings kwarg that will override the text prompt input.\n    if text_embeddings is None:\n        if isinstance(prompt, str):\n            batch_size = 1\n        elif isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n\n        if text_input_ids.shape[-1] &gt; self.tokenizer.model_max_length:\n            removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n            print(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n            )\n            text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n        text_embeddings = self.text_encoder(text_input_ids.to(self.device))[0]\n    else:\n        batch_size = text_embeddings.shape[0]\nHere‚Äôs the full code for our StableDiffusionWalkPipeline:\n\nimport inspect\nfrom typing import Optional, Union, List, Callable\n\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n\nclass StableDiffusionWalkPipeline(StableDiffusionPipeline):\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        height: int = 512,\n        width: int = 512,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n        text_embeddings: Optional[torch.FloatTensor] = None,\n    ):\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps &lt;= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if text_embeddings is None:\n            if isinstance(prompt, str):\n                batch_size = 1\n            elif isinstance(prompt, list):\n                batch_size = len(prompt)\n            else:\n                raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n            # get prompt text embeddings\n            text_inputs = self.tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n\n            if text_input_ids.shape[-1] &gt; self.tokenizer.model_max_length:\n                removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n                print(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n            text_embeddings = self.text_encoder(text_input_ids.to(self.device))[0]\n        else:\n            batch_size = text_embeddings.shape[0]\n\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale &gt; 1.0\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance:\n            # HACK - Not setting text_input_ids here when walking, so hard coding to max length of tokenizer\n            # TODO - Determine if this is OK to do\n            # max_length = text_input_ids.shape[-1]\n            max_length = self.tokenizer.model_max_length\n            uncond_input = self.tokenizer(\n                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n            )\n            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n        # get the initial random noise unless the user supplied it\n\n        # Unlike in other pipelines, latents need to be generated in the target device\n        # for 1-to-1 results reproducibility with the CompVis implementation.\n        # However this currently doesn't work in `mps`.\n        latents_device = \"cpu\" if self.device.type == \"mps\" else self.device\n        latents_shape = (batch_size, self.unet.in_channels, height // 8, width // 8)\n        if latents is None:\n            latents = torch.randn(\n                latents_shape,\n                generator=generator,\n                device=latents_device,\n                dtype=text_embeddings.dtype,\n            )\n        else:\n            if latents.shape != latents_shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n            latents = latents.to(latents_device)\n\n        # set timesteps\n        self.scheduler.set_timesteps(num_inference_steps)\n\n        # Some schedulers like PNDM have timesteps as arrays\n        # It's more optimized to move all timesteps to correct device beforehand\n        timesteps_tensor = self.scheduler.timesteps.to(self.device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (Œ∑) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to Œ∑ in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        for i, t in enumerate(self.progress_bar(timesteps_tensor)):\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n            # predict the noise residual\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n            # compute the previous noisy sample x_t -&gt; x_t-1\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n            # call the callback, if provided\n            if callback is not None and i % callback_steps == 0:\n                callback(i, t, latents)\n\n        latents = 1 / 0.18215 * latents\n        image = self.vae.decode(latents).sample\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n        image = image.cpu().permute(0, 2, 3, 1).numpy()\n\n        safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n        image, has_nsfw_concept = self.safety_checker(\n            images=image, clip_input=safety_checker_input.pixel_values.to(text_embeddings.dtype)\n        )\n\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n\nRemove existing pipeline instance before proceeding‚Ä¶\n\ndel pipeline\ntorch.cuda.empty_cache()\n\nThen, initialize the pipeline just as we did before\n\npipeline = StableDiffusionWalkPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision='fp16'\n).to(\"cuda\")\n\nGreat! Now we‚Äôll interpolate between text embeddings. We use torch.lerp instead of slerp, as that‚Äôs what we‚Äôve found to be a bit smoother for text.\nWe‚Äôll start by creating two helper functions, embed_text and get_noise so this repetitive code doesn‚Äôt muddy up our logic below.\n\ndef embed_text(pipeline, text):\n    \"\"\"takes in text and turns it into text embeddings\"\"\"\n    text_input = pipeline.tokenizer(\n        text,\n        padding=\"max_length\",\n        max_length=pipeline.tokenizer.model_max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    with torch.no_grad():\n        embed = pipeline.text_encoder(text_input.input_ids.to(pipeline.device))[0]\n    return embed\n\ndef get_noise(pipeline, seed, height=512, width=512):\n    \"\"\"Takes in random seed and returns corresponding noise vector\"\"\"\n    return torch.randn(\n        (1, pipeline.unet.in_channels, height // 8, width // 8),\n        generator=torch.Generator(\n            device=pipeline.device\n        ).manual_seed(seed),\n        device=pipeline.device,\n    )\n\n\n# Height and width of image are important for noise vector creation\n# Values should be divisible by 8 if less than 512\n# Values should be divisible by 64 if greater than 512\nheight, width = 512, 512\n\n# Prompts/random seeds for A and B\nprompt_a, prompt_b = 'blueberry spaghetti', 'strawberry spaghetti'\nseed_a, seed_b = 42, 1337\n\n# Noise for A and B\nnoise_a = get_noise(pipeline, seed_a, height=height, width=width)\nnoise_b = get_noise(pipeline, seed_b, height=height, width=width)\n\n# Text embeddings for A and B\nembed_a = embed_text(pipeline, prompt_a)\nembed_b = embed_text(pipeline, prompt_b)\n\n\nfrom pathlib import Path\n\noutput_dir = Path('images_walk_with_text')\noutput_dir.mkdir(exist_ok=True, parents=True)\n\n# Steps to interpolate (i.e. number of images to generate)\nnum_interpolation_steps = 10\nT = torch.linspace(0.0, 1.0, num_interpolation_steps).to(pipeline.device)\n\nimages = []\nfor i, t in enumerate(T):\n    noise_t = slerp(float(t), noise_a, noise_b)\n    embed_t = torch.lerp(embed_a, embed_b, t)\n    im = pipeline(\n        text_embeddings=embed_t,\n        latents=noise_t,\n        height=height,\n        width=width\n    )['sample'][0]\n    im.save(output_dir / f'frame{i:06d}.png')\n\n\n! ffmpeg -f image2 -framerate 5 -i images_walk_with_text/frame%06d.png -loop 0 output_2.gif\n\n\n\n\noutput_2.gif"
  },
  {
    "objectID": "posts/sd_music_videos.html#interpolating-to-the-beat-of-a-song",
    "href": "posts/sd_music_videos.html#interpolating-to-the-beat-of-a-song",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Interpolating to the Beat of a Song",
    "text": "Interpolating to the Beat of a Song\nNow we‚Äôre talking! But how might we now move this video to the beat of a song?\nAbove, we were moving between images linearly. What we want to do is:\n\nmove more when the energy of a given audio clip is high (it‚Äôs loud)\nmove less when the energy is low (it‚Äôs quiet).\n\nWe can achieve this by manipulating the weights at certain timesteps that we defined above as T. Instead of using torch.linspace, we‚Äôll try to set these values based on some audio.\nHere we define a helper function to visualize numpy arrays. We‚Äôll use this to help explain what we‚Äôre doing.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef plot_array(y):\n    x = np.arange(y.shape[0]) \n    \n    # plotting\n\n    plt.title(\"Line graph\") \n\n    plt.xlabel(\"X axis\") \n\n    plt.ylabel(\"Y axis\") \n\n    plt.plot(x, y, color =\"red\") \n    return plt.show()\n\nNow let‚Äôs load in an audio clip. The one we‚Äôre using is the choice example clip from librosa.\nIt‚Äôs a good one because it has drums and bass in it, so it‚Äôs similar to a song you might want to use (but doesn‚Äôt involve us using copyrighted music in this notebook üòâ).\nWe‚Äôll slice the audio clip so we are only using audio from 0:11-0:14 in the audio.\n\nimport librosa\nfrom IPython.display import Audio\n\nn_mels = 512\nfps = 12\noffset = 11\nduration = 3\n\nwav, sr = librosa.load(librosa.example('choice'), offset=offset, duration=duration)\nAudio(wav, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nLet‚Äôs take a look at the plot of the waveform:\n\nplot_array(wav)\n\n\n\n\n\n\n\n\nAfter much experimentation, I found that extracting the percussive elements from the song and using those for everything moving forward leads to the best results.\nWe‚Äôll do this using the librosa.effects.hpss function.\n\nwav_harmonic, wav_percussive = librosa.effects.hpss(wav, margin=(1.0, 5.0))\nplot_array(wav_percussive)\n\n\n\n\n\n\n\n\nAs you can see, now the points of percussive impact are more pronounced.\nWhat we‚Äôll do next is:\n\nConvert that audio to spectrogram\nNormalize the spectrogram\nRescale the spectrogram to be (duration * fps) so we have a vector that‚Äôs the same length as the amount of frames we wish to generate.\nPlot the resulting array so we can see what it looks like\n\n\n# Number of audio samples per frame\nframe_duration = int(sr / fps)\n\n# Generate Mel Spectrogram\nspec_raw = librosa.feature.melspectrogram(y=wav_percussive, sr=sr, n_mels=n_mels, hop_length=frame_duration)\n\n# Obtain maximum value per time-frame\nspec_max = np.amax(spec_raw, axis=0)\n\n# Normalize all values between 0 and 1\nspec_norm = (spec_max - np.min(spec_max)) / np.ptp(spec_max)\n\n# rescale so its exactly the number of frames we want to generate\n# 3 seconds at 12 fps == 36\namplitude_arr = np.resize(spec_norm, int(duration * fps))\n\nplot_array(amplitude_arr)\n\n\n\n\n\n\n\n\nFinally, we‚Äôll construct T. We could do this in a variety of ways, but the simplest we found was using np.cumsum to gather a cumulative sum of the ‚Äúenergy‚Äù in the audio array.\nHat tip to @teddykoker who helped me figure this out.\n\n# Cumulative sum of audio energy\nT = np.cumsum(amplitude_arr)\n\n# Normalize values of T against last element\nT /= T[-1]\n\n# 0th element not always exactly 0.0. Enforcing that here.\nT[0] = 0.0\n\nplot_array(T)\n\n\n\n\n\n\n\n\nCompare the above T with our previous definition of T‚Ä¶it‚Äôs a lot different!\nWe can see the one above is increasing rapidly at points of high energy, while the one below is simply linear.\n\nplot_array(np.linspace(0.0, 1.0, fps*duration))\n\n\n\n\n\n\n\n\nLet‚Äôs use our newly defined T to generate our music video!\nüìù Note - This cell will take a little while as it has to do the number of steps you see in X-axis above (36 frames)\n\nfrom pathlib import Path\n\noutput_dir = Path('images_walk_with_audio')\noutput_dir.mkdir(exist_ok=True, parents=True)\n\nfor i, t in enumerate(T):\n    noise_t = slerp(float(t), noise_a, noise_b)\n    embed_t = torch.lerp(embed_a, embed_b, t)\n    im = pipeline(\n        text_embeddings=embed_t,\n        latents=noise_t,\n        height=height,\n        width=width\n    )['sample'][0]\n    im.save(output_dir / f'frame{i:06d}.png')\n\nLet‚Äôs stitch together the frames we just made as well as the audio clip that goes with it.\nFirst, we‚Äôll write that audio clip to a new file, audio.wav.\n\nimport soundfile as sf\n\nsf.write(output_dir / 'audio.wav', wav, samplerate=sr)\n\nThen, we‚Äôll use some ffmpeg witchcraft to stitch the frames together into an mp4, including the audio clip we just wrote.\n\n! ffmpeg \\\n  -r {fps} \\\n  -i images_walk_with_audio/frame%06d.png \\\n  -i images_walk_with_audio/audio.wav \\\n  -c copy \\\n  -map 0:v:0 \\\n  -map 1:a:0 \\\n  -acodec aac \\\n  -vcodec libx264 \\\n  -pix_fmt yuv420p \\\n  output_walk_with_audio.mp4\n\nLet‚Äôs take a look at the video we just made. In Colab, you‚Äôll have to run some code like this. Otherwise, you can just download the file and open it on your computer.\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\ndef visualize_video_colab(video_path):\n    mp4 = open(video_path,'rb').read()\n    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n    return HTML(\"\"\"\n    &lt;video width=400 controls&gt;\n        &lt;source src=\"%s\" type=\"video/mp4\"&gt;\n    &lt;/video&gt;\n    \"\"\" % data_url)\n\nvisualize_video_colab('output_walk_with_audio.mp4')\n\n\n    \n        \n    \n    \n\n\nSuccess!! üî•"
  },
  {
    "objectID": "posts/sd_music_videos.html#parting-tips",
    "href": "posts/sd_music_videos.html#parting-tips",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Parting Tips",
    "text": "Parting Tips\n\nThe quality of the interpolation is better with higher fps. I‚Äôve been using 30 for most of my serious runs, but 60 is probably even better.\nThe workflow I tend to use is:\n\nGenerate images with random seeds, saving them along with the seeds I used.\nPick prompt/seed pairs you like, and use those to generate the videos. That way you know at a high level what the video will look like.\n\nYou can mess with the margin kwarg in librosa.effects.hpss to increase/decrease the intensity of the effect we made here.\nThere are TONs of ways to do what we did here, this is just one way. Feel free to experiment with creating your own T.\nThe NSFW filter can be too liberal at times when generating videos. I tend to remove it when I‚Äôm generating videos for myself, but it‚Äôs a good idea to keep it in when you‚Äôre sharing your videos with others. Please do this responsibly."
  },
  {
    "objectID": "posts/sd_music_videos.html#conclusion",
    "href": "posts/sd_music_videos.html#conclusion",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Conclusion",
    "text": "Conclusion\nToday we saw how to create music videos using Stable Diffusion! If you want a nice interface for doing this, you should check out the Stable Diffusion Videos repo, where all of this is wrapped up nicely for you (Check the README for instructions on how to use it for music videos).\nIf you found this notebook helpful:\n\nconsider giving this repo a star ‚≠êÔ∏è\nconsider following me on Github @nateraw\n\nThanks for reading! Cheers üçª"
  },
  {
    "objectID": "posts/training_musicgen_songstarter.html",
    "href": "posts/training_musicgen_songstarter.html",
    "title": "Why and How I trained MusicGen Songstarter",
    "section": "",
    "text": "This post will bring you through my motivations and process for training MusicGen Songstarter, a fine-tuned MusicGen model that produces useful samples for music producers.\nThis is not a research paper. However, much like a research paper, there is a bit of a long winded background below here to give you some context. Skip to the next section if you want to get straight to the point."
  },
  {
    "objectID": "posts/training_musicgen_songstarter.html#my-background-in-music",
    "href": "posts/training_musicgen_songstarter.html#my-background-in-music",
    "title": "Why and How I trained MusicGen Songstarter",
    "section": "My Background in Music",
    "text": "My Background in Music\nBefore I got into programming, I used to spend my free time producing music. I spent countless hours in middle school downloading various software onto the family computer and infecting it with viruses (sorry Mom!). Nobody I knew at school was into that sort of thing, so I had to go online to find folks like me who were tinkering with music production. Once I found a few friends via [what I imagine were] Soundcloud DMs, we formed a Facebook group to chat, collaborate, share our music, and ask technical questions.\nThis was one of my first real experiences with an online, collaborative community. It was such a beautiful thing. Through collaborating with folks in this group, I was ‚Äúsigned‚Äù to a couple record labels and released tracks on Beatport. In hindsight, the ‚Äúlabels‚Äù were probably just some dudes in their basement taking advantage of kids like me, but it was still fun to get my music out there.\nAs time went on, I got less interested in EDM and more into instrumental hip-hop/experimental music. I started producing, mixing, and mastering for local artist friends of mine. I stopped trying to market my music and just made music for myself, sometimes sharing links with friends and family. Felt more fun this way. At some point along the way, I started ramping up learning programming and ML, and basically stopped making music altogether. Pretty lame!\nHere‚Äôs of my favorite tracks I made (Yes, I realize how wild it is to link to my Soundcloud in a ML blogpost üòÇ)\n\n\n\nnate raw ¬∑ thoughts"
  },
  {
    "objectID": "posts/training_musicgen_songstarter.html#ai-music-research-the-musicgen-weights-release",
    "href": "posts/training_musicgen_songstarter.html#ai-music-research-the-musicgen-weights-release",
    "title": "Why and How I trained MusicGen Songstarter",
    "section": "AI music research + the MusicGen weights release",
    "text": "AI music research + the MusicGen weights release\nI may have put my DJing career on the backburner, but over the years I‚Äôve tried to keep up with the intersection of AI and music. When I saw the papers for MuLan/MusicLM, I started to get really excited. MusicLM showed a LOT of potential. But of course, Google chose not to release the weights. üò≠\nA year later, in Jan 2024, Meta dropped the code AND weights for MusicGen. This model was a bit simpler in architecture compared to MusicLM - it takes audio and tokenizes it with Encodec, then feeds it to a transformer. They showed how you could control the generation by conditioning on text, melody, metadata, etc. The hype was real!! üî•\nWith the excitement also came some minor frustrations‚Ä¶it was exciting because it was a huge step forward in the field of AI music generation - and the weights were open. Frustrating because the Encodec model used to train the larger variants of MusicGen was for 32khz mono audio. A 44.1k/48k stereo MusicGen would have earth shattering for the AI community. Instead, they teased us a bit by releasing a 48k stereo Encodec, but made it frustrating to use in their training codebase, so nobody could practically train a new MusicGen checkpoint with it (if I‚Äôm mistaken about this, please let me know and I‚Äôll revise!). Even if you could, you‚Äôd need a bunch of compute to make it happen.\nAfter some time, Meta released ‚Äústereo‚Äù variants of the MusicGen models with a bit of a hack to interleave codebooks for left and right channels - still using the mono Encodec under the hood. I‚Äôve heard these described as being ‚Äútoo stereo‚Äù, which I tend to agree with. Gripes aside, these stereo models are still INCREDIBLE and I am so grateful for their release. üôè"
  },
  {
    "objectID": "posts/training_musicgen_songstarter.html#musicgen---will-it-finetune",
    "href": "posts/training_musicgen_songstarter.html#musicgen---will-it-finetune",
    "title": "Why and How I trained MusicGen Songstarter",
    "section": "MusicGen - will it finetune?",
    "text": "MusicGen - will it finetune?\nA few months after the release of MusicGen, when there were still just mono models available, I started wondering: how hard it would be to fine-tune this? Just like with any large pretrained model, you often get the best results for your use case by finetuning on your own data. I had a hunch that finetuning MusicGen on a dataset of music samples could produce some really cool results.\nMy first idea was to fine-tune on specific artists. Basically ‚Äúgenerate a song that sounds like &lt;artist&gt; made it‚Äù. So, I started tinkering with the audiocraft codebase, trying to figure out how the training code worked, how to format the data, etc. For me, the codebase is a bit hard to follow. Nested Hydra configs, a PyTorch trainer suite, flashy, that lives in a different github repo, an experiment manager, dora, that also lives in a different repo. It‚Äôs a lot to take in!!\n\nInitial experiments with fine-tuning on artists\nAfter some exploration, I figured out how to get training running using their codebase, and prepared a dataset of samples from a few artists and started training for the text-to-music task, starting from the musicgen-small checkpoint as that was the largest model I could fit on a Google Colab A100 40gb GPU. For the dataset, since I didn‚Äôt have text captions for the music, I set the captions to be the same every time: ‚Äúan electronic instrumental in the style of ‚Äù.\nThe results were‚Ä¶okay. I had to generate a lot of samples to get some good ones out‚Ä¶most were riddled with artifacts/noise/strange sounds/silence. I also was plagued by a sneaking suspicion that the model was overfitting and just regenerating existing tracks. Here are some clips from the first model I trained, where I used a dataset of songs by Monte Booker:\n\n\n\nText Prompt\n\n\nOutput\n\n\n\n\njazz trumpet over a hip hop beat in the style of montebooker\n\n\n\n\nYour browser does not support the audio element. \n\n\n\n\nreggae chords over a hip hop beat in the style of montebooker\n\n\n\n\nYour browser does not support the audio element. \n\n\n\n\n## Aha moment üí°\n\nThese results felt promising to me! So, I went over to Monte Booker's discord and shared the results, asking for feedback. Immediately, folks started taking the samples I shared and remixing the tracks I sent - chopping them up, adding better drums, etc.\n\nThis made something click for me...why not make the outputs even more usable for producers? Instead of generating full tracks like I was doing, why not generate loops like you'd buy from [Splice](https://splice.com)? This way, I'm not making an \"AI version of the artist\", but rather creating a tool that music producers can use creatively. If done correctly, I'd get rid of most of the ethical concerns I was having while also building a more unique project.\n\nThis would be like the new age of sampling! ü§Ø"
  },
  {
    "objectID": "posts/training_musicgen_songstarter.html#the-plan",
    "href": "posts/training_musicgen_songstarter.html#the-plan",
    "title": "Why and How I trained MusicGen Songstarter",
    "section": "The plan",
    "text": "The plan"
  },
  {
    "objectID": "posts/training_musicgen_songstarter.html#data",
    "href": "posts/training_musicgen_songstarter.html#data",
    "title": "Why and How I trained MusicGen Songstarter",
    "section": "Data",
    "text": "Data\nAll these years, I‚Äôve been subscribed to Splice but not producing much music. As a result, I had accumulated a lot of credits. So, my plan was to download a bunch of loops from Splice and use those to train a new model.\n\nDownloading the data\nNothing too fancy here. I sat at my computer and listened to samples for a long time - listening and purchasing samples carefully. Some key notes from this process:\n\nI wanted to stick with samples that matched my taste. I went for hip hop/rap/electronic melodic loops, as well as some soul/jazz samples.\n\nI had to be careful not to download samples that were too similar to each other. Often times sample packs had complimentary samples that were too similar to each other. I wanted to make sure the model was learning a wide variety of sounds.\nI wanted samples to be 15-30s long (note MusicGen was trained on 30s samples).\nI wanted to avoid drums altogether so the model would un-learn to generate drums.\n\n\nFor the songstarter-v0.1, I used ~600 samples. For songstarter-v0.2, I used around 1700-1800 samples - about 7-8 hours of audio altogether.\n\n\nData prep\nI prepared metadata JSON files following the audiocraft repo‚Äôs examples. I set the tags to be the ‚Äúdescription‚Äù, formatting it like:\n{tag_1}, {tag_1}, ..., {tag_n}, {key}, {bpm} bpm\nFor example:\nhip hop, soul, piano, chords, jazz, neo jazz, G# minor, 140 bpm\nI included bpm and key attributes in the audiocraft JSON metadata files as well (which, looking back, means they were included twice in the input to the model sometimes). I also included the genre if it was available. I left mood and instruments blank.\n\n\nTraining\n\nAudiocraft Woes + rewriting training w/ PyTorch Lightning\nOnce I felt I had a good dataset to start training medium/large models, I would spin up a VM on Lambda, and try to start training with the original training code from audiocraft. Unfortunately, in addition to the codebase being hard to navigate (which I attribute to the nested Hydra configs), I ran into a lot of issues with training itself.\nSpecifically, there were bugs when using FSDP that caused deadlocks. When you‚Äôre spending your own personal $$$, you want the training to be as efficient as possible, so not being able to use FSDP reliably was an issue.\nSo, I rewrote the training loop with PyTorch Lightning, as its a nice tool to use when you‚Äôre trying to focus on the model rather than the training loop. This worked. The one major headache my implementation causes is that the checkpoints are saved different than the original audiocraft checkpoints‚Ä¶but at least it works!\n\nAs a quick aside, I think some of my issues with FSDP in audiocraft may have been H100 specific. There are some lingering issues on their GitHub about it. My implementation also had some issues on H100 that I am not 100% sure are resolved. Had limited time to debug because every hour spent not training was ~$30. I actually ended up burning most of the credits Lambda gave me for this project just on one 12 hour session debugging FSDP issues on an 8xH100. üò≠ The final training run was done on 8xA100 40GB.\n\nThe training code I used is available via my fork of the audiocraft repo here. Note I did not add the validation loop here - I found I was evaluating the model by generating samples and listening to them, so I didn‚Äôt bother with it. Again, I was doing most of this on my own time, so I was moving quickly.\n\n\nTraining details\nI don‚Äôt have the exact details for v0.1, which was a medium melody model, so we‚Äôll just discuss musicgen-songstarter-v0.2.\nv0.2 was fine-tuned on top of facebook/musicgen-stereo-melody-large on a Lambda Labs 8xA100 40gb instance for 10k steps, which took about 6 hours. With FSDP, I could safely fit a batch size of 5 per device, so that‚Äôs what I used (meaning global batch size of 40). I reduced the minimum segment duration to 15s (from the base model‚Äôs 30s). The code is available here, but is undocumented as of now.\nI tried a few different combos of batch size / segment duration to find what made sense for me. In the end, even though we trained on smaller samples, the model can still generate longer samples - at the cost (or benefit, depending who you ask) of the fact they are usually looping."
  },
  {
    "objectID": "posts/training_musicgen_songstarter.html#results",
    "href": "posts/training_musicgen_songstarter.html#results",
    "title": "Why and How I trained MusicGen Songstarter",
    "section": "Results",
    "text": "Results\nI don‚Äôt have any quantitative results to show here. I figured it would be unfair to eval against MusicCaps as an eval, since my model is for a more specific use case. Its prompts weren‚Äôt natural language, but instead lists of tags. Instead, I encourage you to hear what it can do!"
  },
  {
    "objectID": "posts/training_musicgen_songstarter.html#conclusion",
    "href": "posts/training_musicgen_songstarter.html#conclusion",
    "title": "Why and How I trained MusicGen Songstarter",
    "section": "Conclusion",
    "text": "Conclusion\nI‚Äôm really excited about the future of AI Music for music producers. I hope to keep working in this space and provide music producers tools that they can use to elevate their creativity - NOT replace them. After all, what I‚Äôm trying to create here is something I would want to use myself."
  },
  {
    "objectID": "posts/multinode_training_accelerate_azureml.html",
    "href": "posts/multinode_training_accelerate_azureml.html",
    "title": "Multi-Node Training with Hugging Face accelerate and AzureML",
    "section": "",
    "text": "In this guide, we‚Äôll see how you can do multi-node/multi-GPU training on AzureML using Hugging Face accelerate.\nMore specifically, we‚Äôll fine-tune an image classification model from timm on the CIFAR10 dataset. We use this dataset as it is small and works well for getting started.\nPrerequisites:"
  },
  {
    "objectID": "posts/multinode_training_accelerate_azureml.html#define-data-upload-script",
    "href": "posts/multinode_training_accelerate_azureml.html#define-data-upload-script",
    "title": "Multi-Node Training with Hugging Face accelerate and AzureML",
    "section": "Define Data Upload Script",
    "text": "Define Data Upload Script\nHere‚Äôs the data upload script. It simply takes in a path (to a .tar.gz file) and extracts it to output_folder. üìù\n\n%%writefile {src_dir}/read_write_data.py\nimport argparse\nimport os\nimport tarfile\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--input_data\", type=str)\nparser.add_argument(\"--output_folder\", type=str)\nargs = parser.parse_args()\n\n\nfile = tarfile.open(args.input_data)\noutput_path = os.path.join(args.output_folder)\nfile.extractall(output_path)\nfile.close()"
  },
  {
    "objectID": "posts/multinode_training_accelerate_azureml.html#define-data-upload-job",
    "href": "posts/multinode_training_accelerate_azureml.html#define-data-upload-job",
    "title": "Multi-Node Training with Hugging Face accelerate and AzureML",
    "section": "Define Data Upload Job",
    "text": "Define Data Upload Job\nNow that we have some code to run, we can define the job. The below basically defines:\n\nInputs: The inputs to our script. In our case it‚Äôs a tar.gz file stored at a URL. This will be downloaded when the job runs. We provide it to our script we wrote above via the --input_data flag.\nOutputs: The path where we will save the outputs in our workspace‚Äôs data store. We pass this to --output_folder in our script.\nEnvironment: We use one of AzureML‚Äôs curated environments, which will result in the job starting faster. Later, for the training job, we‚Äôll define a custom environment.\nCompute: We tell the job to run on our cpu-cluster.\n\nAny inputs/outputs you define can be referenced via ${{inputs.&lt;name&gt;}} and ${{outputs.&lt;name&gt;}} in the command, so the values are passed along to the script.\n\n# Input in this case is a URL that will be downloaded\ninputs = {\n    \"pets_zip\": Input(\n        type=AssetTypes.URI_FILE,\n        path=\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\",\n    ),\n}\n\n# Define output data. The resulting path will be used in run.py\noutputs = {\n    \"pets\": Output(\n        type=AssetTypes.URI_FOLDER,\n        path=f\"azureml://subscriptions/{aml_sub}/resourcegroups/{aml_rsg}/workspaces/{aml_ws_name}/datastores/workspaceblobstore/paths/PETS\",\n    )\n}\n\n# Define our job\njob = command(\n    code=src_dir,\n    command=\"python read_write_data.py --input_data ${{inputs.pets_zip}} --output_folder ${{outputs.pets}}\",\n    inputs=inputs,\n    outputs=outputs,\n    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\",\n    compute=cpu_compute_target,\n    experiment_name=experiment_name,\n    display_name='data-prep-step'\n)"
  },
  {
    "objectID": "posts/multinode_training_accelerate_azureml.html#run-data-upload-job",
    "href": "posts/multinode_training_accelerate_azureml.html#run-data-upload-job",
    "title": "Multi-Node Training with Hugging Face accelerate and AzureML",
    "section": "Run Data Upload Job",
    "text": "Run Data Upload Job\nIf everything goes smoothly, the below should launch the data-prep job, and spit out a link for you to watch it run.\nYou only really need to run this job once, and then can reference it as many times as you like in the training step we are going to define in the next section.\n\n# submit the command\nreturned_job = ml_client.jobs.create_or_update(job)\nreturned_job"
  },
  {
    "objectID": "posts/multinode_training_accelerate_azureml.html#define-training-environment",
    "href": "posts/multinode_training_accelerate_azureml.html#define-training-environment",
    "title": "Multi-Node Training with Hugging Face accelerate and AzureML",
    "section": "Define Training Environment",
    "text": "Define Training Environment\nFor the training job, we‚Äôll define a custom training environment, as our dependencies aren‚Äôt included in the curated environments offered by AzureML. We try to pin most of these to very specific versions so the environment won‚Äôt break in the future/if we share it with others.\n\n%%writefile {src_dir}/train_environment.yml\nname: aml-video-accelerate\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.9\n  - numpy\n  - pip\n  - scikit-learn\n  - scipy\n  - pandas\n  - pip:\n    - pyarrow==9.0.0\n    - azure-identity&gt;=1.8.0\n    - transformers==4.24.0\n    - timm==0.6.12\n    - git+https://github.com/huggingface/accelerate.git@5315290b55ea9babd95a281a27c51d87b89d7c85\n    - fire==0.4.0\n    - torchmetrics==0.10.3\n    - av==9.2.0\n    - torch==1.12.1\n    - torchvision==0.13.1\n    - tensorboard\n    - mlflow \n    - setfit\n    - azure-keyvault-secrets\n    - azureml-mlflow\n    - azure-ai-ml\n\nNow we use the conda environment file we just wrote to specify additional dependencies on top of the curated openmpi3.1.2-ubuntu18.04 docker image from AzureML.\nFor more information on creating environments in AzureML SDK v2, check out the docs.\n\n# Define environment from conda specification\ntrain_environment = Environment(\n    name=\"aml-accelerate\",\n    description=\"Custom environment for Accelerate + PytorchVideo training\",\n    conda_file=str(Path(src_dir) / \"train_environment.yml\"),\n    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n)"
  },
  {
    "objectID": "posts/multinode_training_accelerate_azureml.html#define-training-script",
    "href": "posts/multinode_training_accelerate_azureml.html#define-training-script",
    "title": "Multi-Node Training with Hugging Face accelerate and AzureML",
    "section": "Define Training Script",
    "text": "Define Training Script\nFor our training script, we‚Äôre going to use the complete_cv_example.py script from the official accelerate examples on GitHub.\n\n! wget -O {src_dir}/train.py -nc https://raw.githubusercontent.com/huggingface/accelerate/main/examples/complete_cv_example.py"
  },
  {
    "objectID": "posts/multinode_training_accelerate_azureml.html#define-training-job",
    "href": "posts/multinode_training_accelerate_azureml.html#define-training-job",
    "title": "Multi-Node Training with Hugging Face accelerate and AzureML",
    "section": "Define Training Job",
    "text": "Define Training Job\nThe moment of truth! Let‚Äôs see if we can train an image classifier using multiple GPUs across multiple nodes on AzureML ü§û\nHere, we‚Äôll define a job called train-step where we define:\n\nAn input, pets, which points to the data store path where we stored our processed data earlier.\nOur training command, providing the following flags:\n\n--data_dir: supplying the input reference path\n--with_tracking: To make sure we save logs\n--checkpointing_steps epoch: To make sure we are saving checkpoints every epoch\n--output_dir ./outputs: Save to the ./outputs directory, which is a special directory in AzureML meant for saving any artifacts from training.\n\nOur training_environment we defined above.\nThe distribution as PyTorch, specifying process_count_per_instance, which is how many GPUs there are per node. (in our case, 2).\n\nFor more information on how Multi-Node GPU training works on AzureML, you can refer to the docs.\n\n# Define inputs, which in our case is the path from upload_cats_and_dogs.py\ninputs = dict(\n    pets=Input(\n        type=AssetTypes.URI_FOLDER,\n        path=f\"azureml://subscriptions/{aml_sub}/resourcegroups/{aml_rsg}/workspaces/{aml_ws_name}/datastores/workspaceblobstore/paths/PETS/images\",\n    ),\n)\n\n# Define the job!\njob = command(\n    code=src_dir,\n    inputs=inputs,\n    command=\"python train.py --data_dir ${{inputs.pets}} --with_tracking --checkpointing_steps epoch --output_dir ./outputs\",\n    environment=train_environment,\n    compute=gpu_compute_target,\n    instance_count=num_training_nodes,  # In this, only 2 node cluster was created.\n    distribution={\n        \"type\": \"PyTorch\",\n        # set process count to the number of gpus per node\n        # In our case (using Standard_NC12) we have 2 GPUs per node.\n        \"process_count_per_instance\": num_gpus_per_node,\n    },\n    experiment_name=experiment_name,\n    display_name='train-step'\n)"
  },
  {
    "objectID": "posts/multinode_training_accelerate_azureml.html#run-training-job",
    "href": "posts/multinode_training_accelerate_azureml.html#run-training-job",
    "title": "Multi-Node Training with Hugging Face accelerate and AzureML",
    "section": "Run Training Job",
    "text": "Run Training Job\n\n# Run it! üöÄ\ntrain_job = ml_client.jobs.create_or_update(job)\ntrain_job"
  }
]